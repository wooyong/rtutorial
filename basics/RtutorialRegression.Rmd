---
title: "R tutorial - regression"
author: "Wooyong Lee"
header-includes:
- \usepackage{amsfonts,amssymb,amsmath}
- \usepackage{graphicx}
- \usepackage{setspace}
- \usepackage{cleveref}
output:
  html_document:
    toc: false
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r header, echo=FALSE, message=FALSE, warning=FALSE}
Sys.setenv(lang="EN")
```

# Linear regression

### lm function

Let's use this dataset as an example.

```{r}
library(datasets)
head(mtcars) # this prints the first 6 observations of the dataset
```

To regress `mpg` on a constant and `wt`, use `lm` function:

```{r}
regressionFit = lm(mpg ~ wt, data = mtcars)
```

I saved the return value of `lm` into `regressionFit`. It is a complicated object:

```{r}
str(regressionFit)
```

There are familiar names such as `coefficients`, `residuals`, and `fitted.values`. You can access these by money symbol:

```{r}
regressionFit$coefficients
regressionFit$residuals
regressionFit$fitted.values
```

To produce the regression fit, type these:

```{r}
regressionFit
print(regressionFit)
summary(regressionFit)
```

To run a regression without a constant, use a `-1` keyword:

```{r}
regFitWithoutConst = lm(mpg ~ -1 + wt, data=mtcars)
summary(regFitWithoutConst)
```

You can also add other regressors:

```{r}
regressionFit = lm(mpg ~ wt + cyl + disp, data=mtcars)
summary(regressionFit)
```

In `mtcars` dataset, the variable `cyl` has only three values: `4`, `6`, `8`.

You may want to treat `cyl` as a categorical variable.

To do so, regress `mpg` on `factor(cyl)`. The `factor` function transforms numerical values into categorical values.

```{r}
regressionFit = lm(mpg ~ wt + factor(cyl) + disp, data=mtcars)
summary(regressionFit)
```

Suppose you want to add `wt^2` as an additional regressor. There are two ways of doing it. One way is to create another column in the `data.frame`:

```{r}
mtcars$wt2 = mtcars$wt^2 # the data.frame creates the column wt2 and assign the values.
head(mtcars)
summary(lm(mpg ~ wt + wt2, data=mtcars))
```

Another way that does not involve creating a column is:

```{r}
summary(lm(mpg ~ wt + I(wt^2), data=mtcars))
```

You can use the same trick if you want to include the sum of two variables as a regressor, e.g. `I(cyl+disp)`.

What is the function `I()`? The answer is related to what we discuss in the next section.

### The formula

I have been omitting the label for the first argument of `lm`. It is `formula`:

```{r}
lm(formula = mpg ~ wt + disp, data=mtcars)
```

`formula` is a special object that contains an "expression". Note that I did not specify `mpg ~ wt + disp` as string, in which case I had to write `"mpg ~ wt + disp"`. <!-- (I may write `"mpg ~ wt + disp"`, but it will be automatically converted to a formula object). -->

In `formula`, operators like `~` and `+` are treated differently from the usual way. For example, `+` in the formula is not an arithmetic operator but an operator that indicates multiple regressors.

The function `I`, when used for `formula`, tells that operators like `+` and `^` inside `I` are regarded as arithmetic operators and that the result of operations inside `I` is regarded as a regressor.

Therefore, `I(wt^2)` tells that the square of `wt` is a regressor, and `I(cyl+disp)` tells that the sum of `cyl` and `disp` is a regressor. The `+` within `I` is understood as an arithmetic operator and not the operator that indicates two regressors `cyl` and `disp`.

# Heteroskedasticity

The `lm` function uses basic formula for computing standard errors. To compute heteroskedasticity-robust standard errors, use the `sandwich` and the `lmtest` package:

```{r, results='hide', message=FALSE, warning=FALSE}
library(sandwich)
library(lmtest)
```

Let's use the following regression as an example:

```{r}
lfit = lm(formula = mpg ~ wt + disp, data=mtcars)
summary(lfit)
```

Use `vcovHC` function from the `sandwich` package to compute robust standard errors:

```{r}
vcHC = vcovHC(lfit, type = "HC0")
```

Note that I use the return value of `lm` as the first argument of `vcovHC`.

The above command computes the variance-covariance matrix using the estimator

$$
  vcHC = (X'X)^{-1}X'\Omega X(X'X)^{-1}
$$

where $X$ is the matrix of regressors and

$$
  \Omega = \text{diag}(u_1^2, \ldots, u_N^2)
$$

where $u_i$ is the residual for individual $i$.

You can also write `HC1`, `HC2` or `HC3` instead of `HC0` for the `type` argument. These are all finite-sample corrections of the above estimator.

To generate the analog of `summary(lfit)`, use `coeftest` function from the `lmtest` package:

```{r}
coeftest(lfit, vcov. = vcHC)
```

Compare the above with the return value of `lm`. Only the standard errors differ.

```{r}
summary(lfit)
```

# Clustered errors

You may also compute clustered standard errors using `sandwich` package. In doing so, you need to specify which variables represent group indices, for which you need `plm` package.

```{r, results='hide', message=FALSE, warning=FALSE}
library(plm)
```

`plm` function from the `plm` package performs several kinds of panel data regressions. For now, I use `plm` to do exactly what `lm` does (i.e. OLS), except that I also store information on the group indices.

Let's use the following dataset as an example:

```{r}
data("Grunfeld", package = "plm")
head(Grunfeld)
```

Consider the following OLS:

```{r}
pfitl = lm(inv ~ value + capital, data=Grunfeld)
summary(pfitl)
```

The following code produces exactly the same result as OLS:

```{r}
pfit = plm(inv ~ value + capital, model="pooling", index="firm", data=Grunfeld)
summary(pfit)
```

The option `model="pooling"` tells `plm` to run OLS. The option `index="firm"` tells that observations are grouped according to the `firm` variable.

Now I compute clustered standard errors using `vcovHC` function:

```{r}
vcHCcluster = vcovHC(pfit, type = "HC0", cluster = "group")
```

Note that the value for the `cluster` argument, which is `"group"`, is not a variable name in the dataset `Grunfeld`. The option `cluster = "group"` tells that I use the group index stored in `pfit` as the cluster.

The above command computes the estimator

$$
  vcHCcluster = \left(\sum_{k=1}^K X_k'X_k\right)^{-1} \sum_{k=1}^K X_k'U_kU_k'X_k \left(\sum_{k=1}^K X_k'X_k\right)^{-1}
$$

where $X_k$ is the matrix of regressors for the $k$th cluster and $U_k$ is the residual vector for the $k$th cluster.

To produce the regression fit with clustered standard errors, use the `coeftest` function again:

```{r}
coeftest(pfit, vcov. = vcHCcluster)
```

# Probit and logit

You may use `glm` function to run probit and logit regressions. Their usage is very similar to `lm`.

```{r}
# recall:
head(mtcars)

# let's run probit with some random formula. 
probitFit = glm(am ~ mpg + disp, family = binomial(link="probit"), data = mtcars)
probitFit
summary(probitFit)

# let's run logit.
logitFit = glm(am ~ mpg + disp, family = binomial(link="logit"), data = mtcars)
logitFit
summary(logitFit)

```









